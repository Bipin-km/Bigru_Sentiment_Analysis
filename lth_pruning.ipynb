{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "model = keras.models.load_model(\"...../model.h5\")\n",
    "data = np.load(\".........../data.npz\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = tf.convert_to_tensor(data[\"X_train\"], dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(data[\"y_train\"], dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(data[\"X_val\"], dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(data[\"y_val\"], dtype=tf.float32)\n",
    "\n",
    "# Smaller validation subset\n",
    "val_sample_size = 8000\n",
    "X_val = X_val[:val_sample_size]\n",
    "y_val = y_val[:val_sample_size]\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "target_accuracy = 0.90\n",
    "num_epochs = 3\n",
    "batch_size = 1900\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial weights for rewinding (skip embedding and output layers)\n",
    "initial_weights = []\n",
    "for layer in model.layers:\n",
    "    if not isinstance(layer, keras.layers.Embedding) and layer != model.layers[-1]:  # Skip embedding and output layers\n",
    "        for weight in layer.trainable_weights:\n",
    "            initial_weights.append(tf.identity(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, val_dataset):\n",
    "    # Initialize accuracy metric\n",
    "    accuracy = keras.metrics.BinaryAccuracy()\n",
    "    # Iterate over validation dataset\n",
    "    for x, y in val_dataset:\n",
    "        # Predict and update accuracy state\n",
    "        y_pred = model(x)\n",
    "        accuracy.update_state(y, y_pred)\n",
    "    # Return final accuracy result\n",
    "    return accuracy.result().numpy()\n",
    "\n",
    "def get_sparsity(model):\n",
    "    total_params = 0\n",
    "    sparse_params = 0\n",
    "    # Iterate over model layers\n",
    "    for layer in model.layers:\n",
    "        # Check if layer is Dense or Bidirectional\n",
    "        if isinstance(layer, (keras.layers.Dense, keras.layers.Bidirectional)):\n",
    "            # Iterate over trainable weights\n",
    "            for weight in layer.trainable_weights:\n",
    "                total_params += tf.size(weight).numpy()\n",
    "                sparse_params += tf.reduce_sum(tf.cast(tf.abs(weight) < 1e-8, tf.int32)).numpy()\n",
    "    # Calculate and return sparsity percentage\n",
    "    return 100.0 * sparse_params / total_params\n",
    "\n",
    "def create_global_masks(model, prune_rate):\n",
    "    all_weights = []\n",
    "    # Collect all weights from Dense and Bidirectional layers (excluding output layer)\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (keras.layers.Dense, keras.layers.Bidirectional)) and layer != model.layers[-1]:\n",
    "            for weight in layer.trainable_weights:\n",
    "                all_weights.append(tf.abs(weight).numpy().flatten())\n",
    "    # Concatenate all weights and calculate global threshold\n",
    "    all_weights = np.concatenate(all_weights)\n",
    "    global_threshold = np.percentile(all_weights, prune_rate)\n",
    "    \n",
    "    masks = []\n",
    "    # Create masks based on global threshold\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (keras.layers.Dense, keras.layers.Bidirectional)) and layer != model.layers[-1]:\n",
    "            for weight in layer.trainable_weights:\n",
    "                weight_np = weight.numpy()\n",
    "                mask = tf.cast(tf.abs(weight_np) >= global_threshold, tf.float32)\n",
    "                masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "def apply_masks(model, masks):\n",
    "    idx = 0\n",
    "    # Apply masks to model weights\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (keras.layers.Dense, keras.layers.Bidirectional)) and layer != model.layers[-1]:\n",
    "            for weight in layer.trainable_weights:\n",
    "                weight.assign(weight * masks[idx])\n",
    "                idx += 1\n",
    "\n",
    "def rewind_weights(model, initial_weights, masks):\n",
    "    idx = 0\n",
    "    # Rewind weights to initial values and apply masks\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, (keras.layers.Dense, keras.layers.Bidirectional)) and layer != model.layers[-1]:\n",
    "            for weight in layer.trainable_weights:\n",
    "                # Ensure shapes match before multiplication\n",
    "                if weight.shape != initial_weights[idx].shape:\n",
    "                    raise ValueError(f\"Shape mismatch: weight {weight.shape}, initial_weight {initial_weights[idx].shape}\")\n",
    "                weight.assign(initial_weights[idx] * masks[idx])\n",
    "                idx += 1\n",
    "\n",
    "def train_model_with_masks(model, dataset, epochs, masks):\n",
    "    epoch_losses = []\n",
    "    # Train model for specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        batch_losses = []\n",
    "        with tqdm(total=len(dataset), desc=f\"Epoch {epoch+1}/{epochs}\", leave=False) as pbar:\n",
    "            for x_batch, y_batch in dataset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(x_batch, training=True)\n",
    "                    loss = loss_fn(y_batch, y_pred)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                \n",
    "                # Reapply masks after each update\n",
    "                apply_masks(model, masks)\n",
    "                \n",
    "                batch_losses.append(loss.numpy())\n",
    "                pbar.update(1)\n",
    "        # Calculate and print epoch loss\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial pruning rate and maximum pruning rate\n",
    "prune_rate = 10\n",
    "max_prune_rate = 90\n",
    "\n",
    "# Evaluate initial accuracy on validation dataset\n",
    "best_acc = get_accuracy(model, val_dataset)\n",
    "\n",
    "# Initialize best masks and weights\n",
    "best_masks = None\n",
    "best_weights = [tf.identity(w) for w in model.trainable_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pruning loop\n",
    "while prune_rate <= max_prune_rate:\n",
    "    print(f\"\\nPruning {prune_rate}% globally...\")\n",
    "    \n",
    "    # Create global masks based on current prune rate\n",
    "    masks = create_global_masks(model, prune_rate)\n",
    "    \n",
    "    # Rewind weights to initial values and apply masks\n",
    "    rewind_weights(model, initial_weights, masks)\n",
    "    \n",
    "    # Prepare training dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(batch_size)\n",
    "    \n",
    "    # Train model with masks applied\n",
    "    train_model_with_masks(model, train_ds, num_epochs, masks)\n",
    "    \n",
    "    # Evaluate accuracy on validation dataset\n",
    "    acc = get_accuracy(model, val_dataset)\n",
    "    sparsity = get_sparsity(model)\n",
    "    print(f\"Val Accuracy: {acc:.4f} | Sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    # Check if accuracy meets or exceeds target\n",
    "    if acc >= target_accuracy:\n",
    "        # Update best accuracy, masks, and weights\n",
    "        best_acc = acc\n",
    "        best_masks = masks\n",
    "        best_weights = [tf.identity(w) for w in model.trainable_weights]\n",
    "        prune_rate += 5\n",
    "    else:\n",
    "        print(\"Accuracy dropped below threshold, rolling back...\")\n",
    "        # Restore best weights and masks\n",
    "        apply_masks(model, best_masks)\n",
    "        for w, best_w in zip(model.trainable_weights, best_weights):\n",
    "            w.assign(best_w)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Sparsity: {get_sparsity(model):.2f}%\")\n",
    "print(f\"Validation Accuracy: {get_accuracy(model, val_dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test data to tensors\n",
    "X_test = tf.convert_to_tensor(data[\"X_test\"], dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(data[\"y_test\"], dtype=tf.float32)\n",
    "\n",
    "# Create a dataset from the test tensors and batch it\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_test(model, test_dataset):\n",
    " accuracy = keras.metrics.BinaryAccuracy()\n",
    " for x, y in tqdm(test_dataset, desc='testing'):\n",
    "    y_pred = model(x)\n",
    "    accuracy.update_state(y, y_pred)\n",
    " return accuracy.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy_test(model, test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pruned_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bigru_Sentiment_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
