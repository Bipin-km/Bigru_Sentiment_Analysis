{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10816611,"sourceType":"datasetVersion","datasetId":6715624},{"sourceId":264947,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":226652,"modelId":248431},{"sourceId":264949,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":226654,"modelId":248433},{"sourceId":265194,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":226868,"modelId":248652}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T00:45:45.504491Z","iopub.execute_input":"2025-02-22T00:45:45.504771Z","iopub.status.idle":"2025-02-22T00:45:45.513282Z","shell.execute_reply.started":"2025-02-22T00:45:45.504750Z","shell.execute_reply":"2025-02-22T00:45:45.512402Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pruned_student/keras/default/1/fully_pruned_model_latest.h5\n/kaggle/input/teacher/keras/default/1/sentiment_analysis_model.h5\n/kaggle/input/dataset/data.npz\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import layers, models, losses, optimizers\nfrom tensorflow.keras.models import load_model\nfrom tqdm import tqdm\n\n# -----------------------------------------------------------------------------\n# 2. Data Loading and Preprocessing\n# -----------------------------------------------------------------------------\ndata = np.load(\"/kaggle/input/dataset/data.npz\")\nX_train_np, y_train_np = data[\"X_train\"], data[\"y_train\"]\nX_val_np, y_val_np = data[\"X_val\"], data[\"y_val\"]\nX_test_np, y_test_np = data[\"X_test\"], data[\"y_test\"]\n\nX_train = tf.convert_to_tensor(X_train_np, dtype=tf.int32)\ny_train = tf.convert_to_tensor(y_train_np, dtype=tf.float32)\nX_val = tf.convert_to_tensor(X_val_np, dtype=tf.int32)\ny_val = tf.convert_to_tensor(y_val_np, dtype=tf.float32)\nX_test = tf.convert_to_tensor(X_test_np, dtype=tf.int32)\ny_test = tf.convert_to_tensor(y_test_np, dtype=tf.float32)\n\nbatch_size = 1024\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# -----------------------------------------------------------------------------\n# 3. Load Models (Teacher and Student)\n# -----------------------------------------------------------------------------\nteacher_model = load_model('/kaggle/input/teacher/keras/default/1/sentiment_analysis_model.h5')\noriginal_student = load_model('/kaggle/input/pruned_student/keras/default/1/fully_pruned_model_latest.h5')\n\ninputs = tf.keras.Input(shape=(X_train.shape[1],), dtype=tf.int32, name='input_layer')\nx = original_student.get_layer('embedding')(inputs)\nx = original_student.get_layer('bidirectional')(x)\nx = original_student.get_layer('bidirectional_1')(x)\nx = original_student.get_layer('flatten')(x)\nx = original_student.get_layer('dropout')(x)\nx = original_student.get_layer('dense')(x)\nx = original_student.get_layer('dropout_1')(x)\nx = original_student.get_layer('dense_1')(x)\noutputs = original_student.get_layer('dense_2')(x)\nstudent_model = tf.keras.Model(inputs=inputs, outputs=outputs, name='student_with_same_architecture')\n\n# Feature mapping layers\nteacher_dense_map_bigru1 = layers.Dense(64, name='teacher_mapping_bigru1')\nteacher_dense_map_bigru2 = layers.Dense(128, name='teacher_mapping_bigru2')\nstudent_dense_map_bigru1 = layers.Dense(64, name='student_mapping_bigru1')\nstudent_dense_map_bigru2 = layers.Dense(128, name='student_mapping_bigru2')\n\n# -----------------------------------------------------------------------------\n# 5. Feature extraction functions (Teacher and Student)\n# -----------------------------------------------------------------------------\ndef get_teacher_features(images):\n    embedding_output = teacher_model.get_layer('embedding')(images)\n    bigru1_output = teacher_model.get_layer('bidirectional')(embedding_output)\n    t_feat1 = teacher_dense_map_bigru1(bigru1_output)\n    bigru2_output = teacher_model.get_layer('bidirectional_1')(bigru1_output)\n    t_feat2 = teacher_dense_map_bigru2(bigru2_output)\n    return [t_feat1, t_feat2]\n\ndef get_student_features(images):\n    embedding_output = student_model.get_layer('embedding')(images)\n    bigru1_output = student_model.get_layer('bidirectional')(embedding_output)\n    s_feat1 = student_dense_map_bigru1(bigru1_output)\n    bigru2_output = student_model.get_layer('bidirectional_1')(bigru1_output)\n    s_feat2 = student_dense_map_bigru2(bigru2_output)\n    return [s_feat1, s_feat2]\n\n# -----------------------------------------------------------------------------\n# 6. Improved Attention-based Feature Distillation (AFD)\n# -----------------------------------------------------------------------------\nbce_loss = losses.BinaryCrossentropy(from_logits=False)\nmse_loss = losses.MeanSquaredError()\n\ndef attention_distillation_loss(teacher_features, student_features):\n    query = teacher_features\n    key = student_features\n    value = student_features\n    \n    d_k = tf.cast(tf.shape(key)[-1], tf.float32)\n    scale = tf.sqrt(d_k) + 1e-8\n    attn_scores = tf.matmul(query, key, transpose_b=True) / scale\n    attn_scores = tf.nn.softmax(attn_scores, axis=-1)\n    attn_output = tf.matmul(attn_scores, value)\n    return mse_loss(teacher_features, attn_output)\n\ndef distillation_loss(teacher_out, student_out, teacher_feats, student_feats, labels, beta):\n     feat_loss1 = attention_distillation_loss(teacher_feats[0], student_feats[0])\n     feat_loss2 = attention_distillation_loss(teacher_feats[1], student_feats[1])\n     hard_loss = bce_loss(labels, student_out)\n     return hard_loss + beta * (feat_loss1 + feat_loss2)\n\n# -----------------------------------------------------------------------------\n# 7. Training Setup\n# -----------------------------------------------------------------------------\noptimizer = optimizers.Adam(learning_rate=1e-4)\nepochs = 20\nbeta = 0.7\n\npruned_mask = [tf.cast(tf.abs(w) > 0, dtype=tf.float32) for w in student_model.trainable_variables]\ntrain_loss_metric = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\nval_loss_metric = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\nval_accuracy_metric = tf.keras.metrics.BinaryAccuracy('val_accuracy')\n\n# Early stopping parameters\npatience = 3  # Number of epochs to wait for improvement\nmin_delta = 0.001  # Minimum change in validation loss to qualify as improvement\nbest_val_loss = float('inf')  # Track the best validation loss\nwait = 0  # Counter for epochs without improvement\n\n# -----------------------------------------------------------------------------\n# 8. Training Step (Freeze Pruned Weights)\n# -----------------------------------------------------------------------------\n@tf.function\ndef train_step(images, labels):\n    labels = tf.reshape(labels, (-1, 1))\n    \n    with tf.GradientTape() as tape:\n        teacher_out = teacher_model(images, training=False)\n        student_out = student_model(images, training=True)\n        t_feats = get_teacher_features(images)\n        s_feats = get_student_features(images)\n        total_loss = distillation_loss(teacher_out, student_out, t_feats, s_feats, labels, beta)\n\n    grads = tape.gradient(total_loss, student_model.trainable_variables)\n    masked_grads = [grad * mask for grad, mask in zip(grads, pruned_mask)]\n    optimizer.apply_gradients(zip(masked_grads, student_model.trainable_variables))\n    train_loss_metric.update_state(total_loss)\n\n# -----------------------------------------------------------------------------\n# 9. Validation Step\n# -----------------------------------------------------------------------------\n@tf.function\ndef val_step(images, labels):\n    student_out = student_model(images, training=False)\n    loss = bce_loss(labels, student_out)\n    val_loss_metric.update_state(loss)\n    val_accuracy_metric.update_state(labels, student_out)\n\ndef layerwise_trainable_zero_nonzero_params(model):\n    for layer in model.layers:\n        if hasattr(layer, 'trainable_variables'):\n            for var in layer.trainable_variables:\n                total = tf.size(var).numpy()\n                zero = np.sum(var.numpy() == 0)\n                nonzero = total - zero\n                print(f\"Layer: {layer.name} | Total: {total} | Zero: {zero} | Non-Zero: {nonzero}\")\n\n# -----------------------------------------------------------------------------\n# 10. Training Loop with Early Stopping\n# -----------------------------------------------------------------------------\nnum_batches_per_epoch = len(X_train_np) // batch_size\nnum_val_batches = len(X_val_np) // batch_size  # Calculate validation batches\n\n# Define min and max beta values\nbeta_min = 0.4\nbeta_max = 0.8\nepsilon = 1e-8  # Small value to avoid division by zero\n\n# Training Loop\n# Training Loop with Dynamic Beta\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n\n    train_loss_metric.reset_state()\n    val_loss_metric.reset_state()\n    val_accuracy_metric.reset_state()\n\n    # Training loop\n    with tqdm(total=num_batches_per_epoch, desc=\"Training\") as pbar:\n        for step, (images, labels) in enumerate(train_dataset):\n            if step >= num_batches_per_epoch:\n                break\n            train_step(images, labels)\n            pbar.set_postfix(loss=train_loss_metric.result().numpy())\n            pbar.update(1)\n\n    # Validation loop\n    with tqdm(total=num_val_batches, desc=\"Validation\") as pbar:\n        for step, (images, labels) in enumerate(val_dataset):\n            if step >= num_val_batches:\n                break\n            val_step(images, labels)\n            pbar.set_postfix(val_loss=val_loss_metric.result().numpy(), val_accuracy=val_accuracy_metric.result().numpy())\n            pbar.update(1)\n\n    # Compute current validation loss\n    current_val_loss = val_loss_metric.result().numpy()\n\n    # Early stopping logic\n    if current_val_loss < best_val_loss - min_delta:\n        print(f\"Validation loss improved from {best_val_loss:.4f} to {current_val_loss:.4f}\")\n        best_val_loss = current_val_loss\n        wait = 0\n    else:\n        wait += 1\n        print(f\"Validation loss did not improve. Patience: {wait}/{patience}\")\n\n    if wait >= patience:\n        print(f\"Early stopping triggered at epoch {epoch+1}!\")\n        break\n\n    # Update β dynamically based on validation loss\n    beta = beta_min + (beta_max - beta_min) * ((current_val_loss - best_val_loss) / (best_val_loss + epsilon))\n    beta = tf.clip_by_value(beta, beta_min, beta_max)  # Ensure beta stays in range\n\n    print(f\"Updated Beta: {beta.numpy():.4f}\")\n\n    print(f\"Training Loss: {train_loss_metric.result().numpy()}\")\n    print(f\"Validation Loss: {val_loss_metric.result().numpy()}, Validation Accuracy: {val_accuracy_metric.result().numpy()}\")\n    layerwise_trainable_zero_nonzero_params(student_model)\n# -----------------------------------------------------------------------------\n# 11. Save the Final Model\n# -----------------------------------------------------------------------------\n# Save the final student model\nstudent_model.save('/kaggle/working/student_model_final.h5')  # Save as HDF5 file\nprint(\"Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T00:45:51.048786Z","iopub.execute_input":"2025-02-22T00:45:51.049095Z","iopub.status.idle":"2025-02-22T03:06:41.230328Z","shell.execute_reply.started":"2025-02-22T00:45:51.049072Z","shell.execute_reply":"2025-02-22T03:06:41.229439Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6835/6835 [34:20<00:00,  3.32it/s, loss=0.244]\nValidation: 100%|██████████| 1464/1464 [00:57<00:00, 25.51it/s, val_accuracy=0.928, val_loss=0.198]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss improved from inf to 0.1984\nUpdated Beta: 0.4000\nTraining Loss: 0.2436743527650833\nValidation Loss: 0.19840788841247559, Validation Accuracy: 0.9279764890670776\nLayer: embedding | Total: 1920000 | Zero: 3392 | Non-Zero: 1916608\nLayer: bidirectional | Total: 12288 | Zero: 11505 | Non-Zero: 783\nLayer: bidirectional | Total: 12288 | Zero: 11488 | Non-Zero: 800\nLayer: bidirectional | Total: 384 | Zero: 370 | Non-Zero: 14\nLayer: bidirectional | Total: 12288 | Zero: 11346 | Non-Zero: 942\nLayer: bidirectional | Total: 12288 | Zero: 11489 | Non-Zero: 799\nLayer: bidirectional | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: bidirectional_1 | Total: 24576 | Zero: 24058 | Non-Zero: 518\nLayer: bidirectional_1 | Total: 12288 | Zero: 11971 | Non-Zero: 317\nLayer: bidirectional_1 | Total: 384 | Zero: 379 | Non-Zero: 5\nLayer: bidirectional_1 | Total: 24576 | Zero: 23691 | Non-Zero: 885\nLayer: bidirectional_1 | Total: 12288 | Zero: 11709 | Non-Zero: 579\nLayer: bidirectional_1 | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: dense | Total: 16384 | Zero: 16288 | Non-Zero: 96\nLayer: dense | Total: 128 | Zero: 127 | Non-Zero: 1\nLayer: dense_1 | Total: 8192 | Zero: 8023 | Non-Zero: 169\nLayer: dense_1 | Total: 64 | Zero: 56 | Non-Zero: 8\nLayer: dense_2 | Total: 64 | Zero: 0 | Non-Zero: 64\nLayer: dense_2 | Total: 1 | Zero: 0 | Non-Zero: 1\nEpoch 2/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6835/6835 [33:51<00:00,  3.36it/s, loss=0.216]\nValidation: 100%|██████████| 1464/1464 [00:56<00:00, 26.12it/s, val_accuracy=0.928, val_loss=0.2] \n","output_type":"stream"},{"name":"stdout","text":"Validation loss did not improve. Patience: 1/3\nUpdated Beta: 0.4034\nTraining Loss: 0.21630287170410156\nValidation Loss: 0.20007647573947906, Validation Accuracy: 0.9278737902641296\nLayer: embedding | Total: 1920000 | Zero: 3392 | Non-Zero: 1916608\nLayer: bidirectional | Total: 12288 | Zero: 11505 | Non-Zero: 783\nLayer: bidirectional | Total: 12288 | Zero: 11488 | Non-Zero: 800\nLayer: bidirectional | Total: 384 | Zero: 370 | Non-Zero: 14\nLayer: bidirectional | Total: 12288 | Zero: 11346 | Non-Zero: 942\nLayer: bidirectional | Total: 12288 | Zero: 11489 | Non-Zero: 799\nLayer: bidirectional | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: bidirectional_1 | Total: 24576 | Zero: 24058 | Non-Zero: 518\nLayer: bidirectional_1 | Total: 12288 | Zero: 11971 | Non-Zero: 317\nLayer: bidirectional_1 | Total: 384 | Zero: 379 | Non-Zero: 5\nLayer: bidirectional_1 | Total: 24576 | Zero: 23691 | Non-Zero: 885\nLayer: bidirectional_1 | Total: 12288 | Zero: 11709 | Non-Zero: 579\nLayer: bidirectional_1 | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: dense | Total: 16384 | Zero: 16288 | Non-Zero: 96\nLayer: dense | Total: 128 | Zero: 127 | Non-Zero: 1\nLayer: dense_1 | Total: 8192 | Zero: 8023 | Non-Zero: 169\nLayer: dense_1 | Total: 64 | Zero: 56 | Non-Zero: 8\nLayer: dense_2 | Total: 64 | Zero: 0 | Non-Zero: 64\nLayer: dense_2 | Total: 1 | Zero: 0 | Non-Zero: 1\nEpoch 3/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6835/6835 [33:39<00:00,  3.39it/s, loss=0.209]\nValidation: 100%|██████████| 1464/1464 [00:55<00:00, 26.31it/s, val_accuracy=0.928, val_loss=0.202]\n","output_type":"stream"},{"name":"stdout","text":"Validation loss did not improve. Patience: 2/3\nUpdated Beta: 0.4079\nTraining Loss: 0.20894090831279755\nValidation Loss: 0.20232348144054413, Validation Accuracy: 0.9277076721191406\nLayer: embedding | Total: 1920000 | Zero: 3392 | Non-Zero: 1916608\nLayer: bidirectional | Total: 12288 | Zero: 11505 | Non-Zero: 783\nLayer: bidirectional | Total: 12288 | Zero: 11488 | Non-Zero: 800\nLayer: bidirectional | Total: 384 | Zero: 370 | Non-Zero: 14\nLayer: bidirectional | Total: 12288 | Zero: 11346 | Non-Zero: 942\nLayer: bidirectional | Total: 12288 | Zero: 11489 | Non-Zero: 799\nLayer: bidirectional | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: bidirectional_1 | Total: 24576 | Zero: 24058 | Non-Zero: 518\nLayer: bidirectional_1 | Total: 12288 | Zero: 11971 | Non-Zero: 317\nLayer: bidirectional_1 | Total: 384 | Zero: 379 | Non-Zero: 5\nLayer: bidirectional_1 | Total: 24576 | Zero: 23691 | Non-Zero: 885\nLayer: bidirectional_1 | Total: 12288 | Zero: 11709 | Non-Zero: 579\nLayer: bidirectional_1 | Total: 384 | Zero: 358 | Non-Zero: 26\nLayer: dense | Total: 16384 | Zero: 16288 | Non-Zero: 96\nLayer: dense | Total: 128 | Zero: 127 | Non-Zero: 1\nLayer: dense_1 | Total: 8192 | Zero: 8023 | Non-Zero: 169\nLayer: dense_1 | Total: 64 | Zero: 56 | Non-Zero: 8\nLayer: dense_2 | Total: 64 | Zero: 0 | Non-Zero: 64\nLayer: dense_2 | Total: 1 | Zero: 0 | Non-Zero: 1\nEpoch 4/20\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 6835/6835 [34:24<00:00,  3.31it/s, loss=0.205]\nValidation: 100%|██████████| 1464/1464 [00:57<00:00, 25.46it/s, val_accuracy=0.927, val_loss=0.205]","output_type":"stream"},{"name":"stdout","text":"Validation loss did not improve. Patience: 3/3\nEarly stopping triggered at epoch 4!\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import tensorflow as tf\nst_ml = tf.keras.models.load_model('/kaggle/input/stable_kd_model/keras/default/1/student_model_final.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T05:37:21.081478Z","iopub.execute_input":"2025-02-22T05:37:21.081826Z","iopub.status.idle":"2025-02-22T05:37:36.107272Z","shell.execute_reply.started":"2025-02-22T05:37:21.081800Z","shell.execute_reply":"2025-02-22T05:37:36.106602Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"st_ml.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\ntest_loss, test_accuracy = st_ml.evaluate(X_test, y_test, verbose=1)\n\nprint(f\"Testing Loss: {test_loss:.4f}\")\nprint(f\"Testing Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T05:39:13.432086Z","iopub.execute_input":"2025-02-22T05:39:13.432436Z","iopub.status.idle":"2025-02-22T05:49:17.868650Z","shell.execute_reply.started":"2025-02-22T05:39:13.432410Z","shell.execute_reply":"2025-02-22T05:49:17.867911Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m46875/46875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 13ms/step - accuracy: 0.9276 - loss: 0.2042\nTesting Loss: 0.2044\nTesting Accuracy: 0.9274\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"st_ml.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nval_loss, val_accuracy = st_ml.evaluate(X_val, y_val, verbose=1)\n\nprint(f\"Validation Loss: {val_loss:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T05:49:17.869751Z","iopub.execute_input":"2025-02-22T05:49:17.869982Z","iopub.status.idle":"2025-02-22T05:59:21.739342Z","shell.execute_reply.started":"2025-02-22T05:49:17.869961Z","shell.execute_reply":"2025-02-22T05:59:21.738634Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m46875/46875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 13ms/step - accuracy: 0.9277 - loss: 0.2044\nValidation Loss: 0.2047\nValidation Accuracy: 0.9273\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}